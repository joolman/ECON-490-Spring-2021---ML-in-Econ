{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting \n",
    "\n",
    "#### Table of Contents\n",
    "\n",
    "- [Preliminaries](#Preliminaries)\n",
    "- [AdaBoost](#AdaBoost)\n",
    "- [Null Model](#Null-Model)\n",
    "- [Manual Gradient Boosting](#Manual-Gradient-Boosting)\n",
    "- [Gradient Boosting](#Gradient-Boosting)\n",
    "- [Extreme Gradient Boosting](#Extreme-Gradient-Boosting)\n",
    "\n",
    "\n",
    "Today, we shall predict `pct_d_rgdp`.\n",
    "We will fit \n",
    "\n",
    "1. an AdaBoost model\n",
    "2. a manual gradient boosting model\n",
    "3. `skearn`'s gradient boosting model\n",
    "4. and finally `xgboost`'s EXTREME!!!!!!!!!! gradient boosting model\n",
    "\n",
    "\n",
    "We are performing a regression problem.\n",
    "\n",
    "```\n",
    "conda install -c conda-forge xgboost\n",
    "```\n",
    "****************\n",
    "\n",
    "```\n",
    "def r2(yhat, y):\n",
    "    SSres = ((yhat - y)**2).sum()\n",
    "    SStot = ((y - y.mean())**2).sum()\n",
    "    r2 = 1 - SSres/SStot\n",
    "    return r2\n",
    "```\n",
    "\n",
    "We have been using RMSE, MSE, and MAE to evaluate the performance of regression problems. \n",
    "Now we are going to use $R^2$ to take advantage of `sklearn`'s `.score()` method.\n",
    "\n",
    "HOWEVER! Let's grab our metric functions from our helper script so I can make a point!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run metrics.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# processing\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "# algorithms\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('C:/Users/johnj/Documents/Data/aml in econ 02 spring 2021/class data/class_data.pkl')\n",
    "df = df.drop(columns = 'urate_bin').join([\n",
    "    pd.get_dummies(df['urate_bin'], drop_first = True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['pct_d_rgdp']\n",
    "x = df.drop(columns = 'pct_d_rgdp')\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y,\n",
    "                                                   train_size = 2/3,\n",
    "                                                   random_state = 490)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****\n",
    "# Null Model\n",
    "[TOP](#Boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_null = r2(np.mean(y_train), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_null = rmse(sum(y_train)/len(y_train), y_test)\n",
    "rmse_null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***************\n",
    "# AdaBoost\n",
    "[TOP](#Boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_ada = AdaBoostRegressor(base_estimator = DecisionTreeRegressor(max_depth = 1),\n",
    "                           n_estimators = 200, \n",
    "                           learning_rate = 0.5)\n",
    "reg_ada.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we did!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_ada = reg_ada.score(x_test, y_test)\n",
    "r2_ada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What? A negative $R^2$?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_ada = rmse(reg_ada.predict(x_test), y_test)\n",
    "rmse_ada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have overfit our training data. \n",
    "Perhaps we should take a look at some good ol' cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "param_grid = {\n",
    "    'n_estimators': [15, 25, 50, 75],\n",
    "    'learning_rate': 10.**np.arange(-6, -2)\n",
    "}\n",
    "\n",
    "ada_cv = AdaBoostRegressor(base_estimator = DecisionTreeRegressor(max_depth = 1),\n",
    "                          random_state = 490)\n",
    "\n",
    "grid_search = GridSearchCV(ada_cv, param_grid,\n",
    "                          scoring = 'r2',\n",
    "                          cv = 5,\n",
    "                          n_jobs = 10).fit(x_train, y_train)\n",
    "best = grid_search.best_params_\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_ada_best = AdaBoostRegressor(base_estimator = DecisionTreeRegressor(max_depth = 1),\n",
    "                                n_estimators = best['n_estimators'],\n",
    "                                learning_rate = best['learning_rate'])\n",
    "reg_ada_best.fit(x_train, y_train)\n",
    "r2_ada_best = reg_ada_best.score(x_test, y_test)\n",
    "r2_ada_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We explained 1% of the variation in the test data... Yikes..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****\n",
    "# Manual Gradient Boosting\n",
    "[TOP](#Boosting)\n",
    "\n",
    "Our textbook lays out how to manually fit a gradient descent problem. \n",
    "Since the mid-semester feedback told me most students are not reading the textbook, let's demonstrate how to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg1 = DecisionTreeRegressor(max_depth = 2).fit(x_train, y_train)\n",
    "y_train2 = y_train - reg1.predict(x_train)\n",
    "\n",
    "reg2 = DecisionTreeRegressor(max_depth = 2).fit(x_train, y_train2)\n",
    "y_train3 = y_train2 - reg2.predict(x_train)\n",
    "\n",
    "reg3 = DecisionTreeRegressor(max_depth = 2).fit(x_train, y_train3)\n",
    "\n",
    "yhat = sum(reg.predict(x_test) for reg in (reg1, reg2, reg3))\n",
    "r2_manual = r2(yhat, y_test)\n",
    "r2_manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better than AdaBoost...\n",
    "\n",
    "********\n",
    "# Gradient Boosting\n",
    "[TOP](#Boosting)\n",
    "\n",
    "Fortunately, unlike `AdaBoostRegressor()`, `GradientBoostingRegressor()` has early stopping. \n",
    "\n",
    "This means cross-validation is not necessary!! WOOO!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_gb = GradientBoostingRegressor(n_estimators = 200,\n",
    "                         max_depth = 2,\n",
    "                         learning_rate = 0.1,\n",
    "                         validation_fraction = 1/8,\n",
    "                         random_state = 490,\n",
    "                         n_iter_no_change = 4,\n",
    "                                  verbose = 2)\n",
    "reg_gb.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we did!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_gb = reg_gb.score(x_test, y_test)\n",
    "r2_gb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relatively speaking, not to shabby.\n",
    "Not super exciting either.\n",
    "\n",
    "*****\n",
    "# Extreme Gradient Boosting\n",
    "[TOP](#Boosting)\n",
    "\n",
    "Now we get to test to see if extreme gradient boosting is all that it is made out to be!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_train, x_train_test, y_train_train, y_train_test = train_test_split(x_train, y_train, \n",
    "                                                                           train_size = 4/5,\n",
    "                                                                           random_state = 490)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_xgb = xgb.XGBRegressor(n_estimators = 200,\n",
    "                         max_depth = 2,\n",
    "                         learning_rate = 0.1,\n",
    "                         verbosity = 1,\n",
    "                         random_state = 490)\n",
    "reg_xgb.fit(x_train_train, y_train_train,\n",
    "           eval_set = [(x_train_test, y_train_test)],\n",
    "           early_stopping_rounds = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_xgb = reg_xgb.score(x_test, y_test)\n",
    "r2_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, there you have it.\n",
    "\n",
    "Let us sloppily print out these values for a conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('R2 Null', r2_null)\n",
    "print('R2 ada', r2_ada)\n",
    "print('R2 ada cv', r2_ada_best)\n",
    "print('R2 manual gb', r2_manual)\n",
    "print('R2 gb', r2_gb)\n",
    "print('R2 xgb', r2_xgb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
