{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "expanded-stanley",
   "metadata": {},
   "source": [
    "## Support Vector Machines - Part 1\n",
    "\n",
    "#### Table of Contents\n",
    "\n",
    "- [Preliminaries](#Preliminaries)\n",
    "- [Null Model](#Null-Model)\n",
    "- [10% Correlation](#10%-Correlation)\n",
    "- [5% Correlation](#5%-Correlation)\n",
    "- [1% Correlation](#1%-Correlation)\n",
    "- [Comparison](#Comparison)\n",
    "\n",
    "***\n",
    "# Preliminaries\n",
    "[TOP](#Support-Vector-Machines---Part-1)\n",
    "\n",
    "Here we have our usual set up.\n",
    "\n",
    "However, this time we are going to compare choosing features based upon their correlation with the label `pos_net_job`.\n",
    "We will do so at\n",
    "\n",
    "* 10%\n",
    "- 5%\n",
    "- 1%\n",
    "\n",
    "This will result with a postponed train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scheduled-designer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# processing\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# algorithms\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unsigned-textbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('C:/Users/johnj/Documents/Data/aml in econ 02 spring 2021/class data/class_data.pkl')\n",
    "df_prepped = df.drop(columns = ['urate_bin', 'year']).join([\n",
    "    pd.get_dummies(df['urate_bin'], drop_first = True),\n",
    "    pd.get_dummies(df.year, drop_first = True)    \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environmental-bangkok",
   "metadata": {},
   "source": [
    "**********\n",
    "# Null Model \n",
    "[TOP](#Support-Vector-Machines---Part-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anticipated-banana",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_prepped['pos_net_jobs'].astype(float)\n",
    "y_train, y_test = train_test_split(y, train_size = 2/3, random_state = 490)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joined-mainstream",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_null = y_train.value_counts().index[0]\n",
    "acc_null = np.mean(y_test == yhat_null)\n",
    "acc_null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insured-edward",
   "metadata": {},
   "source": [
    "*****\n",
    "# 10% Correlation\n",
    "[TOP](#Support-Vector-Machines---Part-1)\n",
    "\n",
    "First, let's produce a correlation matrix with the data frame method `.corr()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opening-arrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prepped.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "north-comment",
   "metadata": {},
   "source": [
    "This is far too much information. \n",
    "We reall only want the values for `pos_net_jobs`.\n",
    "\n",
    "Remember that Python is zero-indexed..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plain-baltimore",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prepped.corr().iloc[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secondary-fossil",
   "metadata": {},
   "source": [
    "Now we are going to select those that have at least a 10% correlation with our label. \n",
    "Specifically, we want the absolute value of the correlation to be weakly greater than 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legitimate-people",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_net_job_cor = np.abs(df_prepped.corr().iloc[:, 1])\n",
    "vrbls = pos_net_job_cor[pos_net_job_cor >= 0.10].index\n",
    "vrbls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norman-syria",
   "metadata": {},
   "source": [
    "Neat.\n",
    "\n",
    "Now we can select the variables that we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-bibliography",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prepped2 = df_prepped.loc[:, vrbls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-compensation",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_prepped2['pos_net_jobs'].astype(float)\n",
    "x = df_prepped2.drop(columns = 'pos_net_jobs')\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 2/3, random_state = 490)\n",
    "\n",
    "ss = StandardScaler()\n",
    "x_train_std = pd.DataFrame(ss.fit(x_train).transform(x_train),\n",
    "                           columns = x_train.columns,\n",
    "                           index = x_train.index)\n",
    "\n",
    "x_test_std = pd.DataFrame(ss.fit(x_test).transform(x_test),\n",
    "                          columns = x_test.columns, \n",
    "                          index = x_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neutral-storm",
   "metadata": {},
   "source": [
    "Now let's cross-validate the optimal value of `C`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "starting-supervision",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "param_grid = {\n",
    "    'C': 10.0**np.linspace(-5, 2, num = 20)\n",
    "}\n",
    "\n",
    "svc_cv = LinearSVC(dual = False)\n",
    "\n",
    "grid_search = GridSearchCV(svc_cv, param_grid, \n",
    "                          cv = 5,\n",
    "                          scoring = 'accuracy')\n",
    "grid_search.fit(x_train_std, y_train)\n",
    "best_10 = grid_search.best_params_\n",
    "best_10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing-regulation",
   "metadata": {},
   "source": [
    "Alternatively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legitimate-florida",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "param_grid = { # List or numpy array\n",
    "    'C': 10.0**np.linspace(-5, 2, num = 20),\n",
    "    'dual': [False]\n",
    "}\n",
    "\n",
    "svc_cv = LinearSVC()\n",
    "\n",
    "grid_search = GridSearchCV(svc_cv, param_grid, \n",
    "                          cv = 5,\n",
    "                          scoring = 'accuracy')\n",
    "grid_search.fit(x_train_std, y_train)\n",
    "best_10 = grid_search.best_params_\n",
    "best_10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-opening",
   "metadata": {},
   "source": [
    "Now to refit and find the accuracy with the model with the full testing data using the optimal value of `C`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classical-spokesman",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_tuned_10 = LinearSVC(C = best_10['C'], dual = False)\n",
    "acc_tuned_10 = svc_tuned_10.fit(x_train_std, y_train).score(x_test_std, y_test)\n",
    "acc_tuned_10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlikely-singing",
   "metadata": {},
   "source": [
    "*****\n",
    "# 5% Correlation\n",
    "[TOP](#Support-Vector-Machines---Part-1)\n",
    "\n",
    "Let's do the same thing with a weakly greater than 5% threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bound-metropolitan",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_net_job_cor = np.abs(df_prepped.corr().iloc[:, 1])\n",
    "vrbls = pos_net_job_cor[pos_net_job_cor >= 0.05].index\n",
    "df_prepped2 = df_prepped.loc[:, vrbls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anticipated-quality",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_prepped2['pos_net_jobs'].astype(float)\n",
    "x = df_prepped2.drop(columns = 'pos_net_jobs')\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 2/3, random_state = 490)\n",
    "\n",
    "ss = StandardScaler()\n",
    "x_train_std = pd.DataFrame(ss.fit(x_train).transform(x_train),\n",
    "                           columns = x_train.columns,\n",
    "                           index = x_train.index)\n",
    "\n",
    "x_test_std = pd.DataFrame(ss.fit(x_test).transform(x_test),\n",
    "                          columns = x_test.columns, \n",
    "                          index = x_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-astronomy",
   "metadata": {},
   "source": [
    "Now let's cross-validate the optimal value of `C`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decent-profit",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "param_grid = {\n",
    "    'C': 10.0**np.linspace(-5, 2, num = 20)\n",
    "}\n",
    "\n",
    "svc_cv = LinearSVC(dual = False)\n",
    "\n",
    "grid_search = GridSearchCV(svc_cv, param_grid, \n",
    "                          cv = 5,\n",
    "                          scoring = 'accuracy')\n",
    "grid_search.fit(x_train_std, y_train)\n",
    "best_5 = grid_search.best_params_\n",
    "best_5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-guest",
   "metadata": {},
   "source": [
    "Now to refit and find the accuracy with the model with the full testing data using the optimal value of `C`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "internal-stanford",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_tuned_5 = LinearSVC(C = best_5['C'], dual = False)\n",
    "acc_tuned_5 = svc_tuned_5.fit(x_train_std, y_train).score(x_test_std, y_test)\n",
    "acc_tuned_5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "israeli-corporation",
   "metadata": {},
   "source": [
    "*****\n",
    "# 1% Correlation\n",
    "[TOP](#Support-Vector-Machines---Part-1)\n",
    "\n",
    "Let's do the same thing with a weakly greater than 1% threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-blast",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_net_job_cor = np.abs(df_prepped.corr().iloc[:, 1])\n",
    "vrbls = pos_net_job_cor[pos_net_job_cor >= 0.01].index\n",
    "df_prepped2 = df_prepped.loc[:, vrbls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "searching-aberdeen",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_prepped2['pos_net_jobs'].astype(float)\n",
    "x = df_prepped2.drop(columns = 'pos_net_jobs')\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 2/3, random_state = 490)\n",
    "\n",
    "ss = StandardScaler()\n",
    "x_train_std = pd.DataFrame(ss.fit(x_train).transform(x_train),\n",
    "                           columns = x_train.columns,\n",
    "                           index = x_train.index)\n",
    "\n",
    "x_test_std = pd.DataFrame(ss.fit(x_test).transform(x_test),\n",
    "                          columns = x_test.columns, \n",
    "                          index = x_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-billion",
   "metadata": {},
   "source": [
    "Now let's cross-validate the optimal value of `C`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharing-google",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "param_grid = {\n",
    "    'C': 10.0**np.linspace(-5, 2, num = 20)\n",
    "}\n",
    "\n",
    "svc_cv = LinearSVC(dual = False)\n",
    "\n",
    "grid_search = GridSearchCV(svc_cv, param_grid, \n",
    "                          cv = 5,\n",
    "                          scoring = 'accuracy')\n",
    "grid_search.fit(x_train_std, y_train)\n",
    "best_1 = grid_search.best_params_\n",
    "best_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-medicine",
   "metadata": {},
   "source": [
    "Now to refit and find the accuracy with the model with the full testing data using the optimal value of `C`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supported-kentucky",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_tuned_1 = LinearSVC(C = best_1['C'], dual = False)\n",
    "acc_tuned_1 = svc_tuned_1.fit(x_train_std, y_train).score(x_test_std, y_test)\n",
    "acc_tuned_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupational-motorcycle",
   "metadata": {},
   "source": [
    "********************\n",
    "# Comparison \n",
    "[TOP](#Support-Vector-Machines---Part-1)\n",
    "\n",
    "Print the percent improvement in the accuracy for each of three models. \n",
    "Which model was the best performer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-disposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_10 = 100*(acc_tuned_10 - acc_null)/acc_null\n",
    "pct_5  = 100*(acc_tuned_5  - acc_null)/acc_null\n",
    "pct_1  = 100*(acc_tuned_1  - acc_null)/acc_null\n",
    "\n",
    "print('10% Corr. Accuracy Improvement: {0: .2f}'.format(pct_10))\n",
    "print('5% Corr. Accuracy Improvement: {0: .2f}'.format(pct_5))\n",
    "print('1% Corr. Accuracy Improvement: {0: .2f}'.format(pct_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-virgin",
   "metadata": {},
   "source": [
    "Print the optimal value of `C` for each model. \n",
    "Which model has the least amount of regularization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vertical-animal",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_10['C'])\n",
    "print(best_5['C'])\n",
    "print(best_1['C'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
