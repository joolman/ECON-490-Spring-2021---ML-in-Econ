{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "expressed-opinion",
   "metadata": {},
   "source": [
    "## Unsupervised Learning Part 2\n",
    "\n",
    "#### Table of Contents\n",
    "\n",
    "- [Perliminaries](#Preliminaries)\n",
    "- [Optimal k](#Optimal-K)\n",
    "    - [Inertia Elbow](#Inertia-Elbow)\n",
    "    - [Silhouette Coefficient](#Silhouette-Coefficient)\n",
    "- [Clustering Large Data](#Clustering-Large-Data)\n",
    "\n",
    "*********************\n",
    "# Preliminaries\n",
    "[TOP](#Unsupervised-Learning-Part-2)\n",
    "\n",
    "Here is some setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-sarah",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Processing\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "# alogirthms\n",
    "from sklearn.linear_model import LinearRegression as lm\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans, AgglomerativeClustering\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-change",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_map = pd.DataFrame(plt.rcParams['axes.prop_cycle'])\n",
    "\n",
    "def clust_avg(data, labels):\n",
    "    df_clst_avg = data.copy()\n",
    "    df_clst_avg['Cluster'] = labels\n",
    "    df_clst_avg = df_clst_avg.groupby('Cluster').mean().transpose()\n",
    "    return df_clst_avg\n",
    "    \n",
    "def clust_plot(data_plot, df_clst_avg, labels, cmap):\n",
    "    colors = color_map.iloc[labels].to_numpy().flatten()\n",
    "    \n",
    "    _, ax = plt.subplots(figsize = (8, 4.5))\n",
    "\n",
    "    data_plot.plot(legend = False,\n",
    "             color = colors,\n",
    "             alpha = 0.25,\n",
    "              ax = ax)\n",
    "\n",
    "    df_clst_avg.plot(ax = ax, \n",
    "                    linewidth = 3)\n",
    "    \n",
    "    plt.ylabel('Stadardized Covid Cases')\n",
    "    plt.title('One Year of COVID: Weekly New Cases')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-franchise",
   "metadata": {},
   "source": [
    "Let's load in the data from last lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "antique-index",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('state covid.csv',\n",
    "                index_col = 0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-month",
   "metadata": {},
   "source": [
    "We also need the plotting version of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retained-brother",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = df.transpose()\n",
    "df_plot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-security",
   "metadata": {},
   "source": [
    "*****************\n",
    "# Optimal k\n",
    "[TOP](#Unsupervised-Learning-Part-2)\n",
    "\n",
    "We are going to consider 2-9 clusters for our state COVID data.\n",
    "It will help us out by defining a range a head of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-procedure",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plot = range(2, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "soviet-globe",
   "metadata": {},
   "source": [
    "********\n",
    "## Inertia Elbow\n",
    "[TOP](#Unsupervised-Learning-Part-2)\n",
    "\n",
    "\n",
    "In order to plot the inertia for different values of $k$, we need to fit a specific `KMeans()` for the different values of $k$.\n",
    "Let's use some fancy list comprehensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cubic-hampton",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_grid = [KMeans(n_clusters = k,\n",
    "                     random_state = 490).fit(df)\n",
    "              for k in x_plot]\n",
    "inertias = [kmean.inertia_ for kmean in kmeans_grid]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-giant",
   "metadata": {},
   "source": [
    "Now to produce the figure from lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separate-emerald",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16/3, 9/3))\n",
    "plt.plot(x_plot, inertias, marker = 'o')\n",
    "\n",
    "plt.ylabel('Inertia')\n",
    "plt.xlabel('$k$')\n",
    "plt.title('K-Means Ineratia for different $k$')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('inertia', dpi = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-martial",
   "metadata": {},
   "source": [
    "Where is the elbow? If I had to guess, I would say either at 3 or 5.\n",
    "Why don't we ask our good ol friend $R^2$?\n",
    "\n",
    "We are going to define a function that we can call to grab the $R^2$ values for our different linear piecwise functions.\n",
    "First, let's outline the function in a code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contrary-terrorist",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg = pd.DataFrame({'inertia': inertias,\n",
    "                     'k': range(2,10)})\n",
    "inflection = np.zeros(n)\n",
    "inflection[5:] = 5\n",
    "df_reg['inflection'] = inflection\n",
    "df_reg['k_inflection'] = df_reg['k']*df_reg['inflection']\n",
    "df_reg\n",
    "\n",
    "y = df_reg['inertia']\n",
    "x = df_reg.drop(columns = 'inertia')\n",
    "\n",
    "r2 = lm().fit(x, y).score(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesbian-porter",
   "metadata": {},
   "source": [
    "Now, for our function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diverse-poster",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_inertia(inertias, x_plot):\n",
    "    df_reg = pd.DataFrame({'inertia': inertias,\n",
    "                         'k': x_plot})\n",
    "    n = len(x_plot)\n",
    "    r2 = {}\n",
    "    for k in range(n)[1:(n - 1)]:\n",
    "        inflection = np.zeros(n)\n",
    "        inflection[k:] = k\n",
    "        df_reg['inflection'] = inflection\n",
    "        df_reg['k_inflection'] = df_reg['k']*df_reg['inflection']\n",
    "        df_reg\n",
    "\n",
    "        y = df_reg['inertia']\n",
    "        x = df_reg.drop(columns = 'inertia')\n",
    "\n",
    "        r2[x_plot[k]] = lm().fit(x, y).score(x, y)\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rotary-animation",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2s = r2_inertia(inertias, x_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-castle",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_k = max(r2s, key = r2s.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-encoding",
   "metadata": {},
   "source": [
    "Remember that our smallest value of $k$ was 2, so we need to account for that when obtaining the optimal model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threatened-record",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = kmeans_grid[opt_k - 2].labels_\n",
    "\n",
    "df_avg = clust_avg(df, labels)\n",
    "\n",
    "clust_plot(df_plot, df_avg, labels, color_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initial-montana",
   "metadata": {},
   "source": [
    "***********************\n",
    "## Silhouette Coefficient\n",
    "[TOP](#Unsupervised-Learning-Part-2)\n",
    "\n",
    "To obtain the vector of silhouette coefficients, we will use another list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-account",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_score = [silhouette_score(df, kmean.labels_)\n",
    "          for kmean in kmeans_grid] # Cannot have 1 label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-entrepreneur",
   "metadata": {},
   "source": [
    "Look, another lecture figure!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opening-uzbekistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16/3, 9/3))\n",
    "plt.plot(range(2, 10), s_score, marker = 'o')\n",
    "\n",
    "plt.xlabel('$k$')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('K-Means Silhoutte Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('silhouette', dpi = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "homeless-prize",
   "metadata": {},
   "source": [
    "Inertia says two. Let's see what that looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distributed-journalist",
   "metadata": {},
   "outputs": [],
   "source": [
    "clust = KMeans(n_clusters = 2,\n",
    "              random_state = 490).fit(df)\n",
    "labels = clust.labels_\n",
    "\n",
    "df_avg = clust_avg(df, labels)\n",
    "\n",
    "clust_plot(df_plot, df_avg, labels, color_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaptive-concord",
   "metadata": {},
   "source": [
    "*********\n",
    "# Clustering Large Data\n",
    "[TOP](#Unsupervised-Learning-Part-2)\n",
    "\n",
    "I would never perform this clustering application, however, I am using it to demonstrate the techniques.\n",
    "\n",
    "We are not going to standardize our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adverse-polymer",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_class = pd.read_pickle('C:/Users/johnj/Documents/Data/aml in econ 02 spring 2021/class data/class_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excited-baptist",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prepped = df_class.drop(columns = ['year', 'urate_bin']).join([\n",
    "    pd.get_dummies(df_class['year'], drop_first = True),\n",
    "    pd.get_dummies(df_class['urate_bin'], drop_first = True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thousand-simulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prepped.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-joyce",
   "metadata": {},
   "source": [
    "We will begin by fitting a mini-batch K-means with 500 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-aurora",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbkm = MiniBatchKMeans(n_clusters = 500,\n",
    "                       random_state = 490,\n",
    "                      max_iter = 100,\n",
    "                      batch_size = 100).fit(df_prepped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-volunteer",
   "metadata": {},
   "source": [
    "Now we will obtain the centroids to produce a new data set that we will use to cluster at a lower level with cosine dissimilarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overall-toolbox",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clusters = pd.DataFrame(mbkm.cluster_centers_,\n",
    "                          columns = df_prepped.columns)\n",
    "df_clusters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "terminal-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_dist = cosine_distances(df_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sweet-fireplace",
   "metadata": {},
   "source": [
    "K-means does not support non-euclidean distances. \n",
    "Consequently, we will use agglomerative clustering.\n",
    "\n",
    "We will use a list comprehension much like before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frank-amber",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_grid = [AgglomerativeClustering(n_clusters = k,\n",
    "                                  linkage = 'average',\n",
    "                                  affinity = 'precomputed').fit(cos_dist)\n",
    "           for k in x_plot]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "level-reward",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_score = [silhouette_score(df_clusters, agg.labels_)\n",
    "          for agg in agg_grid] # Cannot have 1 label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-pittsburgh",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_plot, s_score, marker = 'o')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raising-approach",
   "metadata": {},
   "source": [
    "It looks like 3 clusters is the best. Note the negative score values. What does this mean for the performance of our clustering?\n",
    "\n",
    "If we want to return these clusters back to the original values, we can make merging keys with data frames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-billion",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_upper = pd.DataFrame({'i_index': range(df_prepped.shape[0]),\n",
    "                        'upper_cluster': mbkm.labels_})\n",
    "df_upper.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "useful-screen",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lower = pd.DataFrame({'upper_cluster': range(df_clusters.shape[0]),\n",
    "                         'lower_cluster': agg_grid[3-2].labels_})\n",
    "df_lower.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limited-extension",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df_upper, df_lower, on = 'upper_cluster')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "literary-monkey",
   "metadata": {},
   "source": [
    "Finally, suppose we are using another clustering algorithm to aggregate our clusters-such as Birch-which doesn't produce centroids.\n",
    "Here is how you can manually obtain them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "musical-facing",
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_clusters = pd.DataFrame({'cluster': mbkm.labels_})\n",
    "pd.concat([df_prepped.reset_index(drop = True),\n",
    "           upper_clusters],\n",
    "          axis = 1).groupby('cluster').mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
