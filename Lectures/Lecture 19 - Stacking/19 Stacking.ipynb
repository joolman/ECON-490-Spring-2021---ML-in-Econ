{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking\n",
    "\n",
    "#### Table of Contents\n",
    "\n",
    "- [Preliminaries](#Preliminaries)\n",
    "- [Base Learners](#Base-Learners)\n",
    "    - [Ridge](#Ridge)\n",
    "    - [KNN](#KNN)\n",
    "    - [RF](#RF)\n",
    "    - [Best Base Learner](#Best-Base-Learner)\n",
    "- [Average](#Average)\n",
    "- [Weighted Average](#Weighted-Average)\n",
    "- [Model 1: OLS Average](#Model-1:-OLS-Average)\n",
    "- [Model 2: RF Aggregation](#Model-2:-RF-Aggregation)\n",
    "\n",
    "```\n",
    "def rmse(yhat, y):\n",
    "    import numpy as np\n",
    "    RMSE = np.sqrt(np.mean(  (yhat - y)**2  ))\n",
    "    return RMSE\n",
    "\n",
    "def acc(yhat, y):\n",
    "    import numpy as np\n",
    "    acc = np.mean(yhat == y)\n",
    "    return acc\n",
    "\n",
    "def r2(yhat, y):\n",
    "    SSres = ((yhat - y)**2).sum()\n",
    "    SStot = ((y - y.mean())**2).sum()\n",
    "    r2 = 1 - SSres/SStot\n",
    "    return r2\n",
    "\n",
    "def stdz(vector):\n",
    "    import numpy as np\n",
    "    std_vec = (vector - np.mean(vector))/np.std(vector)\n",
    "    return std_vec\n",
    "```\n",
    "\n",
    "*************\n",
    "# Preliminaries\n",
    "[TOP](#Stacking)\n",
    "\n",
    "We will be using the following base learners predicting `pct_d_rgdp` in an ensemble using the aggregation techniques listed in the table of contents:\n",
    "\n",
    "1. Ridge Regression\n",
    "2. KNN\n",
    "3. RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run metrics.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "\n",
    "# Processing\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split\n",
    "\n",
    "# algorithms\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('C:/Users/johnj/Documents/Data/aml in econ 02 spring 2021/class data/class_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to exclude the fixed effect features for `year` to reduce the number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prepped = df.drop(columns = ['urate_bin', 'year']).join([\n",
    "    pd.get_dummies(df['urate_bin'], drop_first = True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to make the choice of standardizing all of our variables.\n",
    "\n",
    "Remember, we need to obtain the data for\n",
    "\n",
    "- `train1`\n",
    "- `train2`\n",
    "- `test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_prepped['pct_d_rgdp']\n",
    "x = df_prepped.drop(columns = 'pct_d_rgdp')\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y,\n",
    "                                                   train_size = 2/3,\n",
    "                                                   random_state = 490)\n",
    "\n",
    "x_train1, x_train2, y_train1, y_train2 = train_test_split(x_train, y_train,\n",
    "                                    train_size = 1/2,\n",
    "                                    random_state = 490)\n",
    "\n",
    "x_train1 = x_train1.apply(stdz)\n",
    "x_train2 = x_train2.apply(stdz)\n",
    "x_test   = x_test.apply(stdz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing what we do not need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%who"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df, df_prepped, x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%who"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***********\n",
    "# Base Learners\n",
    "[TOP](#Stacking)\n",
    "\n",
    "In this demonstration, we are only going to use 3 base learners.\n",
    "However, there is nothing stopping your from using more.\n",
    "In fact, you may find that the more learners you have, the better your model.\n",
    "\n",
    "However, once you start to include a larger number of base learners, you may want to consider using regularization to aggregate their predictions.\n",
    "\n",
    "*************\n",
    "## Ridge\n",
    "[TOP](#Stacking)\n",
    "\n",
    "We will be using a ridge regression function from `sklearn`, which means we do not need to append an intercept to the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_ridge = RidgeCV(alphas = 10.**np.linspace(-2, 5, num = 20),\n",
    "                   cv = 5).fit(x_train1, y_train1)\n",
    "reg_ridge.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_ridge = reg_ridge.score(x_test, y_test)\n",
    "r2_ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***************\n",
    "# KNN\n",
    "[TOP](#Stacking)\n",
    "\n",
    "Remember that KNN is relatively slow at fitting and relatively slow at predicting.\n",
    "All the other models we have used so far are at least relatively fast at predicting.\n",
    "\n",
    "**Why is KNN slow at predicting?** *Hint: it is in its name!*\n",
    "\n",
    "Let the CV begin! We are going to set a hard limit of 100 on the number of neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "param_grid = {\n",
    "    'n_neighbors': [5, 10, 25, 50, 75, 100]\n",
    "}\n",
    "\n",
    "knn_cv = KNeighborsRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(knn_cv, param_grid,\n",
    "                          cv = 5,\n",
    "                          scoring = 'neg_mean_squared_error',\n",
    "                          n_jobs = 10,\n",
    "                          verbose = 2).fit(x_train1, y_train1)\n",
    "best_knn = grid_search.best_params_\n",
    "best_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to refit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_knn = KNeighborsRegressor(n_neighbors = best_knn['n_neighbors'])\n",
    "reg_knn.fit(x_train1, y_train1)\n",
    "\n",
    "r2_knn = reg_knn.score(x_test, y_test)\n",
    "r2_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*************\n",
    "## RF\n",
    "[TOP](#Stacking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "reg_rf = RandomForestRegressor(n_estimators = 500,\n",
    "                              max_features = 'sqrt',\n",
    "                              random_state = 490,\n",
    "                              n_jobs = 10).fit(x_train1, y_train1)\n",
    "r2_rf = reg_rf.score(x_test, y_test)\n",
    "r2_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**************\n",
    "## Best Base Learner\n",
    "[TOP](#Stacking)\n",
    "\n",
    "We can print out the base learners $R^2$ performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_base = {\n",
    "    'r2_ridge': r2_ridge,\n",
    "    'r2_knn': r2_knn,\n",
    "    'r2_rf': r2_rf\n",
    "}\n",
    "print(r2_base, '\\n')\n",
    "best_base = max(r2_base, key = r2_base.get)\n",
    "\n",
    "print(best_base, ':', r2_base[best_base])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**********\n",
    "# Average\n",
    "[TOP](#Stacking)\n",
    "\n",
    "Remember that the coefficients (the wieghts) are predetermined for a simple average. \n",
    "They are specifically set to the inverse of the number of base learners. \n",
    "To see this, let $j$ denote the base learner index.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\bar{f_j}(x) & = \\frac{1}{3}\\sum_{j=1}^3 f_j(x)\\\\\n",
    "    & = \\frac{1}{3}f_1(x) + \\frac{1}{3}f_2(x) + \\frac{1}{3}f_3(x)\\\\\n",
    "    & = w_1 f_1(x) + w_2 f_2(x) + w_3 f_3(x)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "MATH!!!!!\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_yhat = pd.DataFrame({\n",
    "    'ridge': reg_ridge.predict(x_test),\n",
    "#     'svr': reg_svr.predict(x_test),\n",
    "    'knn': reg_knn.predict(x_test),\n",
    "    'rf': reg_rf.predict(x_test)}, \n",
    "index = y_test.index)\n",
    "df_test_yhat.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_avg = r2(df_test_yhat.mean(axis = 1), y_test)\n",
    "r2_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "************\n",
    "# Weighted Average\n",
    "[TOP](#Stacking)\n",
    "\n",
    "In order to estimate a weighted average, we need to create a grid of weights such that they all add to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 0.1\n",
    "wts = np.arange(0, 1 + step_size, step = step_size)\n",
    "wts_grid = np.array([(x, y, z) for x in wts for y in wts for z in wts])\n",
    "\n",
    "keep = wts_grid.sum(axis = 1) == 1\n",
    "wts_grid = wts_grid[keep]\n",
    "\n",
    "wts_grid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to be using the predicted values on `train2` to identify the optimal weights.\n",
    "\n",
    "It is computationally efficient to only estimate them once, so we are going to create a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train2_yhat = pd.DataFrame({\n",
    "    'ridge': reg_ridge.predict(x_train2),\n",
    "    'knn': reg_knn.predict(x_train2),\n",
    "    'rf': reg_rf.predict(x_train2)}, \n",
    "index = y_train2.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to identify the optimal weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_grid = {}\n",
    "\n",
    "i = 0\n",
    "for w in tqdm(wts_grid):\n",
    "    yhat = df_train2_yhat @ w.T\n",
    "    r2_grid[i] = r2(yhat, y_test)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_indx = max(r2_grid, key = r2_grid.get)\n",
    "best_wts = wts_grid[best_indx]\n",
    "best_wts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the $R^2$..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = df_test_yhat @ best_wts.T\n",
    "\n",
    "r2_wtd_avg = r2(yhat, y_test)\n",
    "r2_wtd_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**************\n",
    "# Model 1: OLS Average\n",
    "[TOP](#Stacking)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How is OLS an average?**\n",
    "\n",
    "Well with a slight abuse of notation, recall that in this case OLS takes the form\n",
    "\n",
    "$$\\hat{y} = \\beta_0 + \\hat{y}_1 \\beta_1 + \\hat{y}_2 \\beta_2 + \\hat{y}_3 \\beta_3 $$\n",
    "\n",
    "Here, $\\beta_1$, $\\beta_2$, and $\\beta_3$ are acting as weights that do not sum to 1.\n",
    "$\\beta_0$ is a *bias* term. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_ols = LinearRegression().fit(df_train2_yhat, y_train2)\n",
    "print(stack_ols.intercept_, stack_ols.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_ols.coef_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_stack_ols = stack_ols.score(df_test_yhat, y_test)\n",
    "r2_stack_ols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**************\n",
    "# Model 2: RF Aggregation\n",
    "[TOP](#Stacking)\n",
    "\n",
    "We can also use different models as stackers.\n",
    "\n",
    "Here we will use a random forest. \n",
    "We will use the usual `max_features = 'sqrt'`, however, we will also add `max_depth = 2` because we have so few features. \n",
    "We will also reduce `n_estimators` by an order of magnitude for the same reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_rf = RandomForestRegressor(n_estimators = 50,\n",
    "                                max_features = 'sqrt',\n",
    "                                max_depth = 2,\n",
    "                                random_state = 490,\n",
    "                                n_jobs = 10)\n",
    "stack_rf.fit(df_train2_yhat, y_train2)\n",
    "\n",
    "r2_stack_rf = stack_rf.score(df_test_yhat, y_test)\n",
    "r2_stack_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****************\n",
    "# Comparison\n",
    "[TOP](#Stacking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%whos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
